---
title: "Avocado Price Modelling"
author: Colin Scotland | DE13
output:
  html_document:
    code_folding: hide
    theme: cerulean
    toc: yes
    toc_float: yes
    toc_depth: 4
  pdf_document:
    toc: yes
---

```{r, echo = FALSE, warning=FALSE, message=FALSE}

library(ggfortify)
library(modelr)
library(GGally)
library(mosaic)
library(here)
library(lubridate)
library(janitor)
library(relaimpo)
library(tidyverse)

here::here()
```

```{r}
avocados <- read_csv(here("1_weekend_homework_part1/data/avocado.csv")) %>% 
  janitor::clean_names()
avocados
```

Check for NAs in the data set;

```{r}
avocados %>% 
  summarise(across(.cols = everything(), ~ sum(is.na(.))))
```

So there are no NAs to consider.

## Make a test set

The data consists of > 18k observations, so there should be plenty of capacity
to create a test set.  10% of 18k is around 1800 observations, which sounds like
a reasonable size for a test set.

Make a test index using 10% of the data;
```{r}
avocado_row_count <- nrow(avocados)

test_index <- sample(1:avocado_row_count, size = avocado_row_count * 0.1)
```

Then create the test and train data sets using this index;
```{r}
avo_test <- slice(avocados, test_index)
avo_train <- slice(avocados, -test_index)
```

## Process the Data

#### Seasonality

The discussion relates to the growth and harvest of a fruit, so there's a good 
chance that there's a seasonal element to the price that could be a significant 
factor;
```{r}
avo_train <- avo_train %>% 
  mutate(month = month(as.POSIXlt(date), label = FALSE))
 
```

```{r}
avo_train %>% 
  dplyr::select(month, average_price) %>% 
  group_by(month) %>% 
  summarise(mean_price_per_month = mean(average_price)) %>% 
  ggplot() +
  aes(x = month, y = mean_price_per_month) +
  geom_line(group = 1) +
  geom_point() +
  labs(title = "Mean Average Avocado Price per Month\nJan 2015 - March 2018",
       x = "Month",
       y = "Mean Average Price") +
  theme_bw() + 
  theme(title = element_text(face = "bold"))
```

<br><br>
This plot suggests that there is indeed some seasonality.  It would be worth
retaining the `month` column to check in the model later, but there's no need to 
keep the level of detail contained in the `date` column.  Variation by day is 
too much detail.

#### Regions

Considering `region`, there are several values that are very ambiguous such as 
`plains`, `WestTexNewMexico`, `SouthEast`.  Although regional variations may be
a significant factor worthy of inclusion in a model, it would be difficult to rationally separate all of the areas into discrete constituent states, so region 
will also be removed.

```{r}
avo_train <- avo_train %>% 
  dplyr::select(-c(date, region))
```


#### Other Considerations

The `x1` column appears to be an identifier of individual observations and will
not influence the model, so can be removed safely.
```{r}
avo_train <- avo_train %>% 
  dplyr::select(-x1)
```


It looks likely that there will be aliases in bag sizes;
```{r}
alias(lm(average_price ~ ., data = avo_train))
```

The alias function shows nothing at this point, but it looks like 
`total_bags` = `small_bags` + `large_bags` + `x_large_bags` - verify this
a different way, bearing in mind that R is not a perfect calculator when 
trying to compare floating point numbers;
```{r}
avo_train %>% 
  mutate(bag_check = 
           as.integer(total_bags - (small_bags + large_bags + x_large_bags))) %>% 
  distinct(bag_check)
```


So it looks like the supposition:

`total_bags` = `small_bags` + `large_bags` + `x_large_bags`

is true - R's lack of precision in performing floating point calculations leads
to rounding errors within +/- 1, which is what is shown in the results.

In conclusion, we can safely remove `total_bags` from the data set, because this
can be calculated if necessary.

```{r}
avo_train <- avo_train %>% 
  dplyr::select(-total_bags)
```

`x4046`, `x4225` or `x4770` are Price Lookup Codes:

(from https://loveonetoday.com/how-to/identify-hass-avocados/)

The most commonly sold sizes of fresh Hass avocado can be identified by their 
Price Look Up code or PLU or sticker.

* Small/Medium Hass Avocado (~3-5oz avocado) | #4046 Avocado
* Large Hass Avocado (~8-10oz avocado) | #4225 Avocado
* Extra Large Hass Avocado (~10-15oz avocado) | #4770 Avocado

One last change is to set non-numeric variables as factors for processing;
```{r}
avo_train <- avo_train %>% 
  mutate(type = as.factor(type),
         month = as.factor(month))
```


## First Predictor

Check the remaining variables for signs of correlation in order to determine
possible predictors for the model;

```{r}
ggpairs(avo_train)
```


## mod1a ----

From the plot it appears as though `type` may be an important factor;

```{r}
avo_train %>% 
  ggplot() +
  aes(x = type, y = average_price) +
  geom_boxplot() +
  theme_bw() +
  labs(title = "Average Price v Avocado Type",
       x = "Avocado Type",
       y = "Average Price")
```


`type` is a categorical variable, so should be checked for significance using 
ANOVA both with and without `type` included;
```{r}
anova(lm(average_price ~ . -type, data = avo_train))
```

```{r}
anova(lm(average_price ~ ., data = avo_train))
```


It looks like type *is* statistically significant.  So the first model could be;
```{r}
mod1a <- lm(average_price ~ type, data = avo_train)
```

How does this look;
```{r}
summary(mod1a)
```

This suggests that 37% of the variance in our model could be explained by `type`.

Run the diagnostic plots;
```{r}
autoplot(mod1a, alpha = 0.3)
```

These look acceptable;

* The Residuals v Fitted are spread around 0 with a fairly flat line
* Q-Q plot is not perfect, but is on the line for the most part
* Scale - Location are in a band above the x-axis with only a very slight funnel
effect evident.

## mod1b ----

At the very start it looked like `month` might also be a significant factor due 
to the seasonality of avocado growth and harvesting;

```{r}
mod1b <- lm(average_price ~ month, data = avo_train)
```

Check ANOVA with and without `month`;
```{r}
anova(lm(average_price ~ . -month, data = avo_train))
```



```{r}
anova(mod1b)
```

```{r}
autoplot(mod1b, alpha = 0.3)
```

These don't look perfect, but once again the residuals v fitted are spread 
across 0 with a fairy flat line, and the Q-Q plot is on the line for the mid-
section of the points.

```{r}
summary(mod1b)
```
It looks like some of the months have a significant effect on average_price, 
although the overall R-squared is poor.  If they are to be included then *all* 
months will be included.

Double check significance; run an ANOVA against the null model (intercept only);
```{r}
null_model <- lm(average_price ~ 1, data = avo_train)
month_model <- lm(average_price ~ month, data = avo_train)

anova(null_model, month_model)
```

So `month` is significant.


## mod1c ----

It would make sense that avocado size would impact on price:

```{r}
mod1c <- lm(average_price ~ x4046, data = avo_train)
```


```{r}
autoplot(mod1c, alpha = 0.3)
```

The diagnostic plots really don't look acceptable.  There are signs of non-normal
distribution in every plot.  

------------------------------------------------------------------
Taking the log of `x4046`returned an error; 

Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
NA/NaN/Inf in 'x'
------------------------------------------------------------------


## mod1d ----

Try `x4225`;
```{r}
mod1d <- lm(average_price ~ x4225, data = avo_train)
```


```{r}
autoplot(mod1d, alpha = 0.3)
```



This is also unacceptable; try `x4770`

## mod1e ----

```{r}
mod1e <- lm(average_price ~ x4770, data = avo_train)
```


```{r}
autoplot(mod1d)
```


`x4770` is also unacceptable in isolation.

It looks like avocado size *might* be significant based on the ANOVA results
earlier, but they don't fit the linear model.

## Model 1 ----

So `type` looks like the most promising predictor so far, giving;
```{r}
avo_model1 <- lm(average_price ~ type, data = avo_train)
```


## Second Predictor 

Looking at residuals;
```{r}
avo_residuals <- avo_train %>% 
  add_residuals(mod1a) %>% 
  dplyr::select(-c(average_price, type))

avo_residuals_numeric <- avo_residuals %>%
  dplyr::select_if(is.numeric)

avo_residuals_nonnumeric <- avo_residuals %>%
  dplyr::select_if(function(x) !is.numeric(x))

avo_residuals_nonnumeric$resid <- avo_residuals$resid
```

```{r}
ggpairs(avo_residuals_numeric)
```

```{r}
ggpairs(avo_residuals_nonnumeric)
```

Month(as tried in mod1b) looks like it's the next best against `resid`, although 
`year` also looks like it might be a factor, having the next highest (albeit low)
correlation coefficient with resid.  

## mod2a ----

Add month into the model, since the ANOVA above did show significance.


```{r}
mod2a <- lm(average_price ~ type + month, data = avo_train)
```


```{r}
summary(mod2a)
```


The adjusted R-squared indicates that 43% of the variance in our model could be 
explained by `type` + `month`.

```{r}
autoplot(mod2a, alpha = 0.3)
```

The diagnostic plots are generally acceptable for this model;

* Fitted v Residuals are spread across 0 with a fairly horizontal line.
* Q-Q plot shows around 75% of the points are on the line
* Scale - Location does not show any extreme funnelling effect

How does this compare with using `year` as the second predictor?

## mod2b ----

```{r}
mod2b <- lm(average_price ~ type + year, data = avo_train)
```


```{r}
summary(mod2b)
```

This shows that `year` has less of an effect than `month`.  Make model 2 with 
`month`;

## Model 2 ----

```{r}
avo_model2 <- lm(average_price ~ type + month, data = avo_train)
```


And look at the residuals again;
```{r}
avo_residuals <- avo_train %>% 
  add_residuals(mod1a) %>% 
  dplyr::select(-c(average_price, type, month))

avo_residuals_numeric <- avo_residuals %>%
  dplyr::select_if(is.numeric)

avo_residuals_nonnumeric <- avo_residuals %>%
  dplyr::select_if(function(x) !is.numeric(x))

avo_residuals_nonnumeric$resid <- avo_residuals$resid
```


```{r}
ggpairs((avo_residuals_numeric))
```

Looking at the correlation against `resid` it doesn't look like there are any 
other very highly significant contributors.  All correlation coefficients are
below 0.1 in magnitude.

This suggests that the best model for average pricing of avocados is model 2.

$$ average\_price \sim type + month $$

This indicates that the average price of avocados varies as an approximately
linear function of the type of avocado and the month of the year.  This seems 
fairly reasonable.

The analysis also shows that the model can explain around 43% of the variance in 
avocado average price.

## Model 3: Check for Interactions

The model currently depends on `type` and `month`.  Do they show any interactions?

Separate the residuals;
```{r}
avocado_residuals <- avo_train %>% 
  add_residuals(avo_model2) %>% 
  dplyr::select(-average_price)
```


Visualise the potential interactions with coplot;
```{r}
coplot(average_price ~ type | month, 
       panel = function(x, y, ...){
         points(x, y)
         abline(lm(y ~ x), col = "blue")
       },
       data = avo_train, 
       alpha = 0.3, 
       columns = 6)
```



At first glance it looks like there's no interaction between `type` and `month`. 
The blue line is pretty constant for all ranges of month.

Check the summary;
```{r}
avo_model3 <- lm(average_price ~ type + month + type:month, data = avo_train)

summary(avo_model3)
```


It looks like there is a very small increase in adjusted r-squared when the 
interaction is included; from 43% to 44%, which would give the best fit model as;

$$ average\_price \sim type + month + type:month $$



What are the relative importances of the model constituents;

```{r}
calc.relimp(avo_model3, type = "lmg", rela = TRUE)
```


This shows that 86% of the r-squared value is made up of `type`, 13% is `month`
and the interaction component only makes up 0.5%.  In the interest of simplicity
it would probably be safe to remove the interaction, but the significance of this
could be checked with BIC;

```{r}
BIC(avo_model2)
```
```{r}
BIC(avo_model3)
```

Based on the BIC values, avo_model2 would be the best (without interaction):

$$ average\_price \sim type + month $$


## Run the Test Set

Now try the test set on the model.  The test set needs to be updated to include
the month column to allow the model to run;
```{r}
avo_test %>% 
   mutate(month = month(as.POSIXlt(date), label = TRUE, abbr = TRUE),
          type = as.factor(type),
         month = as.factor(month))
```


```{r}
summary(avo_model2)
```

This gives an adjusted r-squared of 44%, which corresponds well with what the 
result was for the training set (43%).  The fact that the test set is a better fit also indicates that there's no excessive overfitting.



## Summary

Through the analysis detailed above the following conclusions can be drawn;

* The data set was tidied in several ways, namely
    - date was changed from day granularity to month
    - region was discounted due to insufficient clarification of general regional 
 areas such as `plains`, `WestTexNewMexico`and `SouthEast`
* The model was manually developed using a test-train split
* The best fit model found for the average price of avocados gave an adjusted
r-squared of 44% with the test data and was defined as;

$$ average\_price \sim type + month $$

* An interaction between `type` and `month` was found, but was very small (0.5% 
of total r-squared) and was removed after comparing the BIC on models with and
without the interaction
* This model was found to perform equally well on the test data as on the training
data, with no evidence of overfitting



